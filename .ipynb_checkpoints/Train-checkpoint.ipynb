{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907b68e-79cb-4dac-881c-a68445519f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e61b4083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded compiled 3D CUDA chamfer distance\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from models.autoencoder import AutoEncoder\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models.utils import  AverageMeter, str2bool\n",
    "from dataset.dataset import CompressDataset\n",
    "from args.shapenet_args import parse_shapenet_args\n",
    "from args.semantickitti_args import parse_semantickitti_args\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from models.Chamfer3D.dist_chamfer_3D import chamfer_3DDist\n",
    "chamfer_dist = chamfer_3DDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d292010-360d-44fc-9a97-e9f90bd2f36c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    start = time.time()\n",
    "\n",
    "    if args.batch_size > 1:\n",
    "        print('The performance will degrade if batch_size is larger than 1!')\n",
    "\n",
    "    if args.compress_normal == True:\n",
    "        args.in_fdim = 6\n",
    "\n",
    "    # load data\n",
    "    train_dataset = CompressDataset(data_path=args.train_data_path, cube_size=args.train_cube_size, batch_size=args.batch_size)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, batch_size=args.batch_size)\n",
    "\n",
    "    val_dataset = CompressDataset(data_path=args.val_data_path, cube_size=args.val_cube_size, batch_size=args.batch_size)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=args.batch_size)\n",
    "\n",
    "    # set up folders for checkpoints\n",
    "    str_time = datetime.now().isoformat()\n",
    "    print('Experiment Time:', str_time)\n",
    "    checkpoint_dir = os.path.join(args.output_path, str_time, 'ckpt')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    # create the model\n",
    "    model = AutoEncoder(args)\n",
    "    model = model.cuda()\n",
    "    print('Training Arguments:', args)\n",
    "    print('Model Architecture:', model)\n",
    "\n",
    "    # optimizer for autoencoder\n",
    "    parameters = set(p for n, p in model.named_parameters() if not n.endswith(\".quantiles\"))\n",
    "    optimizer = optim.Adam(parameters, lr=args.lr)\n",
    "    # lr scheduler\n",
    "    scheduler_steplr = StepLR(optimizer, step_size=args.lr_decay_step, gamma=args.gamma)\n",
    "    # optimizer for entropy bottleneck\n",
    "    aux_parameters = set(p for n, p in model.named_parameters() if n.endswith(\".quantiles\"))\n",
    "    aux_optimizer = optim.Adam(aux_parameters, lr=args.aux_lr)\n",
    "\n",
    "    # best validation metric\n",
    "    best_val_chamfer_loss = float('inf')\n",
    "\n",
    "   # train\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_loss = AverageMeter()\n",
    "        epoch_chamfer_loss = AverageMeter()\n",
    "        epoch_density_loss = AverageMeter()\n",
    "        epoch_pts_num_loss = AverageMeter()\n",
    "        epoch_latent_xyzs_loss = AverageMeter()\n",
    "        epoch_normal_loss = AverageMeter()\n",
    "        epoch_bpp_loss = AverageMeter()\n",
    "        epoch_aux_loss = AverageMeter()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for i, input_dict in enumerate(train_loader):\n",
    "            # input: (b, n, c)\n",
    "            input = input_dict['xyzs'].cuda()\n",
    "            # input: (b, c, n)\n",
    "            input = input.permute(0, 2, 1).contiguous()\n",
    "\n",
    "            # compress normal\n",
    "            if args.compress_normal == True:\n",
    "                normals = input_dict['normals'].cuda().permute(0, 2, 1).contiguous()\n",
    "                input = torch.cat((input, normals), dim=1)\n",
    "\n",
    "            # model forward\n",
    "            decompressed_xyzs, loss, loss_items, bpp = model(input)\n",
    "            epoch_loss.update(loss.item())\n",
    "            epoch_chamfer_loss.update(loss_items['chamfer_loss'])\n",
    "            epoch_density_loss.update(loss_items['density_loss'])\n",
    "            epoch_pts_num_loss.update(loss_items['pts_num_loss'])\n",
    "            epoch_latent_xyzs_loss.update(loss_items['latent_xyzs_loss'])\n",
    "            epoch_normal_loss.update(loss_items['normal_loss'])\n",
    "            epoch_bpp_loss.update(loss_items['bpp_loss'])\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # update the parameters of entropy bottleneck\n",
    "            aux_loss = model.feats_eblock.loss()\n",
    "            if args.quantize_latent_xyzs == True:\n",
    "                aux_loss += model.xyzs_eblock.loss()\n",
    "            epoch_aux_loss.update(aux_loss.item())\n",
    "\n",
    "            aux_optimizer.zero_grad()\n",
    "            aux_loss.backward()\n",
    "            aux_optimizer.step()\n",
    "\n",
    "            # print loss\n",
    "            if (i+1) % args.print_freq == 0:\n",
    "                print(\"train epoch: %d/%d, iters: %d/%d, loss: %f, avg chamfer loss: %f, \"\n",
    "                      \"avg density loss: %f, avg pts num loss: %f, avg latent xyzs loss: %f, \"\n",
    "                      \"avg normal loss: %f, avg bpp loss: %f, avg aux loss: %f\" %\n",
    "                      (epoch+1, args.epochs, i+1, len(train_loader), epoch_loss.get_avg(), epoch_chamfer_loss.get_avg(),\n",
    "                       epoch_density_loss.get_avg(), epoch_pts_num_loss.get_avg(), epoch_latent_xyzs_loss.get_avg(),\n",
    "                       epoch_normal_loss.get_avg(), epoch_bpp_loss.get_avg(), epoch_aux_loss.get_avg()))\n",
    "\n",
    "        scheduler_steplr.step()\n",
    "\n",
    "        # print loss\n",
    "        interval = time.time() - start\n",
    "        print(\"train epoch: %d/%d, time: %d mins %.1f secs, loss: %f, avg chamfer loss: %f, \"\n",
    "              \"avg density loss: %f, avg pts num loss: %f, avg latent xyzs loss: %f, \"\n",
    "              \"avg normal loss: %f, avg bpp loss: %f, avg aux loss: %f\" %\n",
    "              (epoch+1, args.epochs, interval/60, interval%60, epoch_loss.get_avg(), epoch_chamfer_loss.get_avg(),\n",
    "               epoch_density_loss.get_avg(), epoch_pts_num_loss.get_avg(), epoch_latent_xyzs_loss.get_avg(),\n",
    "               epoch_normal_loss.get_avg(), epoch_bpp_loss.get_avg(), epoch_aux_loss.get_avg()))\n",
    "\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_chamfer_loss = AverageMeter()\n",
    "        val_normal_loss = AverageMeter()\n",
    "        val_bpp = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for input_dict in val_loader:\n",
    "                # xyzs: (b, n, c)\n",
    "                input = input_dict['xyzs'].cuda()\n",
    "                # (b, c, n)\n",
    "                input = input.permute(0, 2, 1).contiguous()\n",
    "\n",
    "                # compress normal\n",
    "                if args.compress_normal == True:\n",
    "                    normals = input_dict['normals'].cuda().permute(0, 2, 1).contiguous()\n",
    "                    input = torch.cat((input, normals), dim=1)\n",
    "                    args.in_fdim = 6\n",
    "\n",
    "                # gt_xyzs\n",
    "                gt_xyzs = input[:, :3, :].contiguous()\n",
    "\n",
    "                # model forward\n",
    "                decompressed_xyzs, loss, loss_items, bpp = model(input)\n",
    "                # calculate val loss and bpp\n",
    "                d1, d2, _, _ = chamfer_dist(gt_xyzs.permute(0, 2, 1).contiguous(),\n",
    "                                            decompressed_xyzs.permute(0, 2, 1).contiguous())\n",
    "                chamfer_loss = d1.mean() + d2.mean()\n",
    "                val_chamfer_loss.update(chamfer_loss.item())\n",
    "                val_normal_loss.update(loss_items['normal_loss'])\n",
    "                val_bpp.update(bpp.item())\n",
    "\n",
    "        # print loss\n",
    "        print(\"val epoch: %d/%d, val bpp: %f, val chamfer loss: %f, val normal loss: %f\" %\n",
    "              (epoch+1, args.epochs, val_bpp.get_avg(), val_chamfer_loss.get_avg(), val_normal_loss.get_avg()))\n",
    "\n",
    "        # save checkpoint\n",
    "        cur_val_chamfer_loss = val_chamfer_loss.get_avg()\n",
    "        if  cur_val_chamfer_loss < best_val_chamfer_loss or (epoch+1) % args.save_freq == 0:\n",
    "            model_name = 'ckpt-best.pth' if cur_val_chamfer_loss < best_val_chamfer_loss else 'ckpt-epoch-%02d.pth' % (epoch+1)\n",
    "            model_path = os.path.join(checkpoint_dir, model_name)\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            # update best val chamfer loss\n",
    "            if cur_val_chamfer_loss < best_val_chamfer_loss:\n",
    "                best_val_chamfer_loss = cur_val_chamfer_loss\n",
    "\n",
    "def reset_model_args(train_args, model_args):\n",
    "    for arg in vars(train_args):\n",
    "        setattr(model_args, arg, getattr(train_args, arg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a8652c-2322-441c-ad67-4d6f60b4947e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'shapenet'\n",
    "        self.lr = 0.001\n",
    "        self.aux_lr = 0.001\n",
    "        self.weight_decay = 0.001\n",
    "        self.betas = (0.9, 0.999)\n",
    "        self.lr_decay_step = 20\n",
    "        self.gamma = 0.5\n",
    "        self.train_data_path = './data/shapenet/shapenet_train_cube_size_22.pkl'\n",
    "        self.train_cube_size = 22\n",
    "        self.val_data_path = './data/shapenet/shapenet_val_cube_size_22.pkl'\n",
    "        self.val_cube_size = 22\n",
    "        self.test_data_path = './data/shapenet/shapenet_test_cube_size_22.pkl'\n",
    "        self.test_cube_size = 22\n",
    "        self.peak = 30\n",
    "        self.epochs = 100\n",
    "        self.batch_size = 32\n",
    "        self.print_freq = 10\n",
    "        self.save_freq = 5\n",
    "        self.output_path = './output'\n",
    "        self.compress_normal = False\n",
    "        self.in_fdim = 3\n",
    "        self.k = 32\n",
    "        self.downsample_rate = [1/2, 1/2, 1/2]\n",
    "        self.max_upsample_num = [4, 4, 4]\n",
    "        self.layer_num = 3\n",
    "        self.dim = 64\n",
    "        self.hidden_dim = 64\n",
    "        self.ngroups = 8\n",
    "        self.quantize_latent_xyzs = True\n",
    "        self.latent_xyzs_conv_mode = 'mlp'\n",
    "        self.sub_point_conv_mode = 'mlp'\n",
    "        self.chamfer_coe = 1.0\n",
    "        self.pts_num_coe = 1.0\n",
    "        self.normal_coe = 1.0\n",
    "        self.bpp_lambda = 1e-3\n",
    "        self.mean_distance_coe = 1.0\n",
    "        self.density_coe = 1.0\n",
    "        self.latent_xyzs_coe = 1.0\n",
    "        self.model_path = './output/model.pth'\n",
    "        self.density_radius = 0.05\n",
    "        self.dist_coe = 1.0\n",
    "        self.omega_xyzs = 1.0\n",
    "        self.omega_normals = 1.0\n",
    "\n",
    "train_args = Arguments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e4b331-e255-4ee9-bc2d-13c07a032645",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance will degrade if batch_size is larger than 1!\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: './data/shapenet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6672/3796175156.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6672/1761376656.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompressDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcube_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_cube_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mahim/5A40E1DF40E1C23D/CodeBase/D-PCC/dataset/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_path, map_size, cube_size, batch_size, points_num)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcube_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './data/shapenet'"
     ]
    }
   ],
   "source": [
    "train(train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba251435-f626-4545-aaf2-6ed1028f8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model_args(train_args, model_args):\n",
    "    for arg in vars(train_args):\n",
    "        setattr(model_args, arg, getattr(train_args, arg))\n",
    "\n",
    "# Create a class to hold model arguments\n",
    "class ModelArgs:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd0d1b9-3ddb-45f8-aee0-009a40f3cac3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d2aa63-e5dc-491c-90f2-4c81235d0d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Training-related parameters (set some default values initially)\n",
    "        self.dataset = 'shapenet'\n",
    "        self.batch_size = 1\n",
    "        self.downsample_rate = [1/3, 1/3, 1/3]\n",
    "        self.max_upsample_num = [8, 8, 8]\n",
    "        self.bpp_lambda = 1e-3\n",
    "        self.compress_normal = False\n",
    "        self.quantize_latent_xyzs = True\n",
    "        self.latent_xyzs_conv_mode = 'mlp'\n",
    "        self.sub_point_conv_mode = 'mlp'\n",
    "        self.output_path = './output'\n",
    "        \n",
    "        # Dataset-related parameters (your original default values)\n",
    "        self.data_root = './'\n",
    "        self.instance_num = 50\n",
    "        self.split_rate = 0.8\n",
    "        self.output_mesh_dir = './data/shapenet/mesh'\n",
    "        self.output_pcd_dir = './data/shapenet/pcd'\n",
    "        self.cube_size = 22\n",
    "        self.train_min_num = 1024\n",
    "        self.test_min_num = 100\n",
    "        self.max_num = 500000\n",
    "\n",
    "def parse_args():\n",
    "    args = Args()\n",
    "    parser = argparse.ArgumentParser(description='Arguments')\n",
    "\n",
    "    # Training-related arguments\n",
    "    parser.add_argument('--dataset', default=args.dataset, type=str, help='shapenet or semantickitti')\n",
    "    parser.add_argument('--batch_size', default=args.batch_size, type=int, help='the performance will degrade if batch_size is larger than 1!')\n",
    "    parser.add_argument('--downsample_rate', default=args.downsample_rate, nargs='+', type=float, help='downsample rate')\n",
    "    parser.add_argument('--max_upsample_num', default=args.max_upsample_num, nargs='+', type=int, help='max upsmaple number, reversely symmetric with downsample_rate')\n",
    "    parser.add_argument('--bpp_lambda', default=args.bpp_lambda, type=float, help='bpp loss coefficient')\n",
    "    parser.add_argument('--compress_normal', default=args.compress_normal, type=str2bool, help='whether compress normals')\n",
    "    parser.add_argument('--quantize_latent_xyzs', default=args.quantize_latent_xyzs, type=str2bool, help='whether compress latent xyzs')\n",
    "    parser.add_argument('--latent_xyzs_conv_mode', default=args.latent_xyzs_conv_mode, type=str, help='latent xyzs conv mode, mlp or edge_conv')\n",
    "    parser.add_argument('--sub_point_conv_mode', default=args.sub_point_conv_mode, type=str, help='sub-point conv mode, mlp or edge_conv')\n",
    "    parser.add_argument('--output_path', default=args.output_path, type=str, help='output path')\n",
    "    \n",
    "    parsed_args = parser.parse_args()\n",
    "\n",
    "    # Update the args object based on the parsed values\n",
    "    args.dataset = parsed_args.dataset\n",
    "    args.batch_size = parsed_args.batch_size\n",
    "    args.downsample_rate = parsed_args.downsample_rate\n",
    "    args.max_upsample_num = parsed_args.max_upsample_num\n",
    "    args.bpp_lambda = parsed_args.bpp_lambda\n",
    "    args.compress_normal = parsed_args.compress_normal\n",
    "    args.quantize_latent_xyzs = parsed_args.quantize_latent_xyzs\n",
    "    args.latent_xyzs_conv_mode = parsed_args.latent_xyzs_conv_mode\n",
    "    args.sub_point_conv_mode = parsed_args.sub_point_conv_mode\n",
    "    args.output_path = parsed_args.output_path\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0626f23-3ddd-4b31-8e6b-0853dfba372c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_parse_args():\n",
    "    # Store the original arguments\n",
    "    original_argv = sys.argv\n",
    "    # Temporarily overwrite sys.argv\n",
    "    sys.argv = ['']\n",
    "    try:\n",
    "        return parse_args()\n",
    "    finally:\n",
    "        # Restore the original arguments\n",
    "        sys.argv = original_argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9868bd78-1f6b-49dd-8ed8-654c4e077e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Args at 0x7f59a495cd10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_args = test_parse_args()\n",
    "train_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aedf3f32-de75-4230-9683-25ead9ee0b3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_args = ModelArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03a3af69-2ae5-464d-ad08-cc46d81199e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ModelArgs at 0x7f59a4987850>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ec41d60-c47a-4df2-a8d6-e60c0dc6e5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reset_model_args(train_args, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f99806f-9978-4aad-9112-d0ec8bbe894c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ModelArgs' object has no attribute 'train_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5594/490779075.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_5594/4001131566.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCompressDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcube_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_cube_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelArgs' object has no attribute 'train_data_path'"
     ]
    }
   ],
   "source": [
    "train(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb7596-6651-4ff9-8eb1-72565932ef54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D-PCC",
   "language": "python",
   "name": "d-pcc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
